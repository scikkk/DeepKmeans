{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e53af6",
   "metadata": {},
   "source": [
    "# Run scDeepCluster on 10X PBMC dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38911249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from scDeepCluster import scDeepCluster,cluster_acc\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "from preprocess import read_dataset, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60af52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28201122410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for repeatability\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc9d56",
   "metadata": {},
   "source": [
    "Setup parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e62117",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter setting\n",
    "'''\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.n_clusters = 0\n",
    "        self.knn = 20\n",
    "        self.resolution = 0.8\n",
    "        self.batch_size = 256\n",
    "        self.data_file = './data/sample_151507.h5'\n",
    "        self.maxiter = 2000\n",
    "        self.pretrain_epochs = 300\n",
    "        self.gamma = 1.\n",
    "        self.sigma = 2.5\n",
    "        self.update_interval = 1\n",
    "        self.tol = 0.001\n",
    "        self.ae_weights = None\n",
    "        self.save_dir = 'results/scDeepCluster/'\n",
    "        self.ae_weight_file = 'AE_weights.pth.tar'\n",
    "        self.final_latent_file = 'final_latent_file.txt'\n",
    "        self.predict_label_file = 'pred_labels.txt'\n",
    "        self.device = 'cuda'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24590d2b",
   "metadata": {},
   "source": [
    "Normalizating and preprocessing count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1ae96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Autoencoder: Successfully preprocessed 1159 genes and 4226 cells.\n",
      "<__main__.Args object at 0x0000028228B55940>\n",
      "(4226, 1159)\n",
      "(4226,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_15896\\3523718491.py:13: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata = sc.AnnData(x)\n"
     ]
    }
   ],
   "source": [
    "data_mat = h5py.File(args.data_file)\n",
    "x = np.array(data_mat['X'])\n",
    "# y is the ground truth labels for evaluating clustering performance\n",
    "if 'Y' in data_mat:\n",
    "    y = np.array(data_mat['Y'])\n",
    "else:\n",
    "    y = None\n",
    "data_mat.close()\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing scRNA-seq read counts matrix\n",
    "adata = sc.AnnData(x)\n",
    "if y is not None:\n",
    "    adata.obs['Group'] = y\n",
    "\n",
    "adata = read_dataset(adata)\n",
    "\n",
    "adata = normalize(adata)\n",
    "\n",
    "input_size = adata.n_vars\n",
    "\n",
    "print(args)\n",
    "\n",
    "print(adata.X.shape)\n",
    "if y is not None:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827573d",
   "metadata": {},
   "source": [
    "Build scDeepCluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d2f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scDeepCluster(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1159, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (_enc_mu): Linear(in_features=200, out_features=16, bias=True)\n",
      "  (_dec_mean): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1159, bias=True)\n",
      "    (1): MeanAct()\n",
      "  )\n",
      "  (_dec_disp): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1159, bias=True)\n",
      "    (1): DispAct()\n",
      "  )\n",
      "  (_dec_pi): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1159, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (zinb_loss): ZINBLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = scDeepCluster(input_dim=adata.n_vars, z_dim=16,encodeLayer=[200], decodeLayer=[64,512], sigma=args.sigma, gamma=args.gamma, device=args.device)\n",
    "\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c272d6",
   "metadata": {},
   "source": [
    "Pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bbcfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining stage\n",
      "Pretrain epoch   1, ZINB loss: 1.25548597\n",
      "Pretrain epoch   2, ZINB loss: 1.04477404\n",
      "Pretrain epoch   3, ZINB loss: 1.01086700\n",
      "Pretrain epoch   4, ZINB loss: 0.99763910\n",
      "Pretrain epoch   5, ZINB loss: 0.99108203\n",
      "Pretrain epoch   6, ZINB loss: 0.98763675\n",
      "Pretrain epoch   7, ZINB loss: 0.98625194\n",
      "Pretrain epoch   8, ZINB loss: 0.98465739\n",
      "Pretrain epoch   9, ZINB loss: 0.98342778\n",
      "Pretrain epoch  10, ZINB loss: 0.98231179\n",
      "Pretrain epoch  11, ZINB loss: 0.98123679\n",
      "Pretrain epoch  12, ZINB loss: 0.98045401\n",
      "Pretrain epoch  13, ZINB loss: 0.97958028\n",
      "Pretrain epoch  14, ZINB loss: 0.97921863\n",
      "Pretrain epoch  15, ZINB loss: 0.97810017\n",
      "Pretrain epoch  16, ZINB loss: 0.97762141\n",
      "Pretrain epoch  17, ZINB loss: 0.97690947\n",
      "Pretrain epoch  18, ZINB loss: 0.97603345\n",
      "Pretrain epoch  19, ZINB loss: 0.97556395\n",
      "Pretrain epoch  20, ZINB loss: 0.97525524\n",
      "Pretrain epoch  21, ZINB loss: 0.97492006\n",
      "Pretrain epoch  22, ZINB loss: 0.97420102\n",
      "Pretrain epoch  23, ZINB loss: 0.97413122\n",
      "Pretrain epoch  24, ZINB loss: 0.97325468\n",
      "Pretrain epoch  25, ZINB loss: 0.97300928\n",
      "Pretrain epoch  26, ZINB loss: 0.97313132\n",
      "Pretrain epoch  27, ZINB loss: 0.97269004\n",
      "Pretrain epoch  28, ZINB loss: 0.97252803\n",
      "Pretrain epoch  29, ZINB loss: 0.97250987\n",
      "Pretrain epoch  30, ZINB loss: 0.97171541\n",
      "Pretrain epoch  31, ZINB loss: 0.97145689\n",
      "Pretrain epoch  32, ZINB loss: 0.97100629\n",
      "Pretrain epoch  33, ZINB loss: 0.97112947\n",
      "Pretrain epoch  34, ZINB loss: 0.97092014\n",
      "Pretrain epoch  35, ZINB loss: 0.97075912\n",
      "Pretrain epoch  36, ZINB loss: 0.97038759\n",
      "Pretrain epoch  37, ZINB loss: 0.97017542\n",
      "Pretrain epoch  38, ZINB loss: 0.97013146\n",
      "Pretrain epoch  39, ZINB loss: 0.97015482\n",
      "Pretrain epoch  40, ZINB loss: 0.96994068\n",
      "Pretrain epoch  41, ZINB loss: 0.96968167\n",
      "Pretrain epoch  42, ZINB loss: 0.96981570\n",
      "Pretrain epoch  43, ZINB loss: 0.96939736\n",
      "Pretrain epoch  44, ZINB loss: 0.96944921\n",
      "Pretrain epoch  45, ZINB loss: 0.96920654\n",
      "Pretrain epoch  46, ZINB loss: 0.96907822\n",
      "Pretrain epoch  47, ZINB loss: 0.96888642\n",
      "Pretrain epoch  48, ZINB loss: 0.96867381\n",
      "Pretrain epoch  49, ZINB loss: 0.96850790\n",
      "Pretrain epoch  50, ZINB loss: 0.96847992\n",
      "Pretrain epoch  51, ZINB loss: 0.96848727\n",
      "Pretrain epoch  52, ZINB loss: 0.96851438\n",
      "Pretrain epoch  53, ZINB loss: 0.96854206\n",
      "Pretrain epoch  54, ZINB loss: 0.96796608\n",
      "Pretrain epoch  55, ZINB loss: 0.96817083\n",
      "Pretrain epoch  56, ZINB loss: 0.96818790\n",
      "Pretrain epoch  57, ZINB loss: 0.96807651\n",
      "Pretrain epoch  58, ZINB loss: 0.96782161\n",
      "Pretrain epoch  59, ZINB loss: 0.96743763\n",
      "Pretrain epoch  60, ZINB loss: 0.96768217\n",
      "Pretrain epoch  61, ZINB loss: 0.96754652\n",
      "Pretrain epoch  62, ZINB loss: 0.96745370\n",
      "Pretrain epoch  63, ZINB loss: 0.96738005\n",
      "Pretrain epoch  64, ZINB loss: 0.96744963\n",
      "Pretrain epoch  65, ZINB loss: 0.96712336\n",
      "Pretrain epoch  66, ZINB loss: 0.96702231\n",
      "Pretrain epoch  67, ZINB loss: 0.96716719\n",
      "Pretrain epoch  68, ZINB loss: 0.96687262\n",
      "Pretrain epoch  69, ZINB loss: 0.96689732\n",
      "Pretrain epoch  70, ZINB loss: 0.96701217\n",
      "Pretrain epoch  71, ZINB loss: 0.96674091\n",
      "Pretrain epoch  72, ZINB loss: 0.96667823\n",
      "Pretrain epoch  73, ZINB loss: 0.96692209\n",
      "Pretrain epoch  74, ZINB loss: 0.96635016\n",
      "Pretrain epoch  75, ZINB loss: 0.96695766\n",
      "Pretrain epoch  76, ZINB loss: 0.96644097\n",
      "Pretrain epoch  77, ZINB loss: 0.96652189\n",
      "Pretrain epoch  78, ZINB loss: 0.96652467\n",
      "Pretrain epoch  79, ZINB loss: 0.96615781\n",
      "Pretrain epoch  80, ZINB loss: 0.96628155\n",
      "Pretrain epoch  81, ZINB loss: 0.96615363\n",
      "Pretrain epoch  82, ZINB loss: 0.96617316\n",
      "Pretrain epoch  83, ZINB loss: 0.96618432\n",
      "Pretrain epoch  84, ZINB loss: 0.96604010\n",
      "Pretrain epoch  85, ZINB loss: 0.96615431\n",
      "Pretrain epoch  86, ZINB loss: 0.96592809\n",
      "Pretrain epoch  87, ZINB loss: 0.96581308\n",
      "Pretrain epoch  88, ZINB loss: 0.96609469\n",
      "Pretrain epoch  89, ZINB loss: 0.96576407\n",
      "Pretrain epoch  90, ZINB loss: 0.96564860\n",
      "Pretrain epoch  91, ZINB loss: 0.96581788\n",
      "Pretrain epoch  92, ZINB loss: 0.96571628\n",
      "Pretrain epoch  93, ZINB loss: 0.96542441\n",
      "Pretrain epoch  94, ZINB loss: 0.96553051\n",
      "Pretrain epoch  95, ZINB loss: 0.96544639\n",
      "Pretrain epoch  96, ZINB loss: 0.96538548\n",
      "Pretrain epoch  97, ZINB loss: 0.96499922\n",
      "Pretrain epoch  98, ZINB loss: 0.96545242\n",
      "Pretrain epoch  99, ZINB loss: 0.96546243\n",
      "Pretrain epoch 100, ZINB loss: 0.96522830\n",
      "Pretrain epoch 101, ZINB loss: 0.96537019\n",
      "Pretrain epoch 102, ZINB loss: 0.96524892\n",
      "Pretrain epoch 103, ZINB loss: 0.96522139\n",
      "Pretrain epoch 104, ZINB loss: 0.96468376\n",
      "Pretrain epoch 105, ZINB loss: 0.96495620\n",
      "Pretrain epoch 106, ZINB loss: 0.96494414\n",
      "Pretrain epoch 107, ZINB loss: 0.96496272\n",
      "Pretrain epoch 108, ZINB loss: 0.96500723\n",
      "Pretrain epoch 109, ZINB loss: 0.96459383\n",
      "Pretrain epoch 110, ZINB loss: 0.96458501\n",
      "Pretrain epoch 111, ZINB loss: 0.96446303\n",
      "Pretrain epoch 112, ZINB loss: 0.96452099\n",
      "Pretrain epoch 113, ZINB loss: 0.96450780\n",
      "Pretrain epoch 114, ZINB loss: 0.96467839\n",
      "Pretrain epoch 115, ZINB loss: 0.96450273\n",
      "Pretrain epoch 116, ZINB loss: 0.96429750\n",
      "Pretrain epoch 117, ZINB loss: 0.96447850\n",
      "Pretrain epoch 118, ZINB loss: 0.96464299\n",
      "Pretrain epoch 119, ZINB loss: 0.96464759\n",
      "Pretrain epoch 120, ZINB loss: 0.96439661\n",
      "Pretrain epoch 121, ZINB loss: 0.96416766\n",
      "Pretrain epoch 122, ZINB loss: 0.96458069\n",
      "Pretrain epoch 123, ZINB loss: 0.96446196\n",
      "Pretrain epoch 124, ZINB loss: 0.96427805\n",
      "Pretrain epoch 125, ZINB loss: 0.96405761\n",
      "Pretrain epoch 126, ZINB loss: 0.96392756\n",
      "Pretrain epoch 127, ZINB loss: 0.96416857\n",
      "Pretrain epoch 128, ZINB loss: 0.96396496\n",
      "Pretrain epoch 129, ZINB loss: 0.96398455\n",
      "Pretrain epoch 130, ZINB loss: 0.96379151\n",
      "Pretrain epoch 131, ZINB loss: 0.96387943\n",
      "Pretrain epoch 132, ZINB loss: 0.96379363\n",
      "Pretrain epoch 133, ZINB loss: 0.96392282\n",
      "Pretrain epoch 134, ZINB loss: 0.96382437\n",
      "Pretrain epoch 135, ZINB loss: 0.96375247\n",
      "Pretrain epoch 136, ZINB loss: 0.96350382\n",
      "Pretrain epoch 137, ZINB loss: 0.96355332\n",
      "Pretrain epoch 138, ZINB loss: 0.96362530\n",
      "Pretrain epoch 139, ZINB loss: 0.96360362\n",
      "Pretrain epoch 140, ZINB loss: 0.96350903\n",
      "Pretrain epoch 141, ZINB loss: 0.96334288\n",
      "Pretrain epoch 142, ZINB loss: 0.96346563\n",
      "Pretrain epoch 143, ZINB loss: 0.96351058\n",
      "Pretrain epoch 144, ZINB loss: 0.96328747\n",
      "Pretrain epoch 145, ZINB loss: 0.96338568\n",
      "Pretrain epoch 146, ZINB loss: 0.96306192\n",
      "Pretrain epoch 147, ZINB loss: 0.96352898\n",
      "Pretrain epoch 148, ZINB loss: 0.96316968\n",
      "Pretrain epoch 149, ZINB loss: 0.96334584\n",
      "Pretrain epoch 150, ZINB loss: 0.96340615\n",
      "Pretrain epoch 151, ZINB loss: 0.96317584\n",
      "Pretrain epoch 152, ZINB loss: 0.96338151\n",
      "Pretrain epoch 153, ZINB loss: 0.96296372\n",
      "Pretrain epoch 154, ZINB loss: 0.96303130\n",
      "Pretrain epoch 155, ZINB loss: 0.96305161\n",
      "Pretrain epoch 156, ZINB loss: 0.96332096\n",
      "Pretrain epoch 157, ZINB loss: 0.96303785\n",
      "Pretrain epoch 158, ZINB loss: 0.96272795\n",
      "Pretrain epoch 159, ZINB loss: 0.96294069\n",
      "Pretrain epoch 160, ZINB loss: 0.96286199\n",
      "Pretrain epoch 161, ZINB loss: 0.96261372\n",
      "Pretrain epoch 162, ZINB loss: 0.96271284\n",
      "Pretrain epoch 163, ZINB loss: 0.96266339\n",
      "Pretrain epoch 164, ZINB loss: 0.96261751\n",
      "Pretrain epoch 165, ZINB loss: 0.96267031\n",
      "Pretrain epoch 166, ZINB loss: 0.96291410\n",
      "Pretrain epoch 167, ZINB loss: 0.96262232\n",
      "Pretrain epoch 168, ZINB loss: 0.96235476\n",
      "Pretrain epoch 169, ZINB loss: 0.96263266\n",
      "Pretrain epoch 170, ZINB loss: 0.96259149\n",
      "Pretrain epoch 171, ZINB loss: 0.96250243\n",
      "Pretrain epoch 172, ZINB loss: 0.96243592\n",
      "Pretrain epoch 173, ZINB loss: 0.96257116\n",
      "Pretrain epoch 174, ZINB loss: 0.96247290\n",
      "Pretrain epoch 175, ZINB loss: 0.96240150\n",
      "Pretrain epoch 176, ZINB loss: 0.96232874\n",
      "Pretrain epoch 177, ZINB loss: 0.96236731\n",
      "Pretrain epoch 178, ZINB loss: 0.96261296\n",
      "Pretrain epoch 179, ZINB loss: 0.96256659\n",
      "Pretrain epoch 180, ZINB loss: 0.96246920\n",
      "Pretrain epoch 181, ZINB loss: 0.96230756\n",
      "Pretrain epoch 182, ZINB loss: 0.96230941\n",
      "Pretrain epoch 183, ZINB loss: 0.96229854\n",
      "Pretrain epoch 184, ZINB loss: 0.96214364\n",
      "Pretrain epoch 185, ZINB loss: 0.96231536\n",
      "Pretrain epoch 186, ZINB loss: 0.96259339\n",
      "Pretrain epoch 187, ZINB loss: 0.96204802\n",
      "Pretrain epoch 188, ZINB loss: 0.96178248\n",
      "Pretrain epoch 189, ZINB loss: 0.96199234\n",
      "Pretrain epoch 190, ZINB loss: 0.96222418\n",
      "Pretrain epoch 191, ZINB loss: 0.96224398\n",
      "Pretrain epoch 192, ZINB loss: 0.96237175\n",
      "Pretrain epoch 193, ZINB loss: 0.96204730\n",
      "Pretrain epoch 194, ZINB loss: 0.96193609\n",
      "Pretrain epoch 195, ZINB loss: 0.96169709\n",
      "Pretrain epoch 196, ZINB loss: 0.96191859\n",
      "Pretrain epoch 197, ZINB loss: 0.96180496\n",
      "Pretrain epoch 198, ZINB loss: 0.96192313\n",
      "Pretrain epoch 199, ZINB loss: 0.96208713\n",
      "Pretrain epoch 200, ZINB loss: 0.96181179\n",
      "Pretrain epoch 201, ZINB loss: 0.96188190\n",
      "Pretrain epoch 202, ZINB loss: 0.96185171\n",
      "Pretrain epoch 203, ZINB loss: 0.96151565\n",
      "Pretrain epoch 204, ZINB loss: 0.96145895\n",
      "Pretrain epoch 205, ZINB loss: 0.96141204\n",
      "Pretrain epoch 206, ZINB loss: 0.96156676\n",
      "Pretrain epoch 207, ZINB loss: 0.96149085\n",
      "Pretrain epoch 208, ZINB loss: 0.96170892\n",
      "Pretrain epoch 209, ZINB loss: 0.96168584\n",
      "Pretrain epoch 210, ZINB loss: 0.96165402\n",
      "Pretrain epoch 211, ZINB loss: 0.96179838\n",
      "Pretrain epoch 212, ZINB loss: 0.96133694\n",
      "Pretrain epoch 213, ZINB loss: 0.96133932\n",
      "Pretrain epoch 214, ZINB loss: 0.96157179\n",
      "Pretrain epoch 215, ZINB loss: 0.96157731\n",
      "Pretrain epoch 216, ZINB loss: 0.96176945\n",
      "Pretrain epoch 217, ZINB loss: 0.96144543\n",
      "Pretrain epoch 218, ZINB loss: 0.96102610\n",
      "Pretrain epoch 219, ZINB loss: 0.96113868\n",
      "Pretrain epoch 220, ZINB loss: 0.96129627\n",
      "Pretrain epoch 221, ZINB loss: 0.96130265\n",
      "Pretrain epoch 222, ZINB loss: 0.96077891\n",
      "Pretrain epoch 223, ZINB loss: 0.96100232\n",
      "Pretrain epoch 224, ZINB loss: 0.96167491\n",
      "Pretrain epoch 225, ZINB loss: 0.96126333\n",
      "Pretrain epoch 226, ZINB loss: 0.96090064\n",
      "Pretrain epoch 227, ZINB loss: 0.96098239\n",
      "Pretrain epoch 228, ZINB loss: 0.96103928\n",
      "Pretrain epoch 229, ZINB loss: 0.96107288\n",
      "Pretrain epoch 230, ZINB loss: 0.96088375\n",
      "Pretrain epoch 231, ZINB loss: 0.96118411\n",
      "Pretrain epoch 232, ZINB loss: 0.96108261\n",
      "Pretrain epoch 233, ZINB loss: 0.96102836\n",
      "Pretrain epoch 234, ZINB loss: 0.96064592\n",
      "Pretrain epoch 235, ZINB loss: 0.96078285\n",
      "Pretrain epoch 236, ZINB loss: 0.96053849\n",
      "Pretrain epoch 237, ZINB loss: 0.96076022\n",
      "Pretrain epoch 238, ZINB loss: 0.96053916\n",
      "Pretrain epoch 239, ZINB loss: 0.96063794\n",
      "Pretrain epoch 240, ZINB loss: 0.96018128\n",
      "Pretrain epoch 241, ZINB loss: 0.96067117\n",
      "Pretrain epoch 242, ZINB loss: 0.96065934\n",
      "Pretrain epoch 243, ZINB loss: 0.96018448\n",
      "Pretrain epoch 244, ZINB loss: 0.96070246\n",
      "Pretrain epoch 245, ZINB loss: 0.96101527\n",
      "Pretrain epoch 246, ZINB loss: 0.96043778\n",
      "Pretrain epoch 247, ZINB loss: 0.96036940\n",
      "Pretrain epoch 248, ZINB loss: 0.96061028\n",
      "Pretrain epoch 249, ZINB loss: 0.96064475\n",
      "Pretrain epoch 250, ZINB loss: 0.96047566\n",
      "Pretrain epoch 251, ZINB loss: 0.96010861\n",
      "Pretrain epoch 252, ZINB loss: 0.96051107\n",
      "Pretrain epoch 253, ZINB loss: 0.96060886\n",
      "Pretrain epoch 254, ZINB loss: 0.96030226\n",
      "Pretrain epoch 255, ZINB loss: 0.96009282\n",
      "Pretrain epoch 256, ZINB loss: 0.96018357\n",
      "Pretrain epoch 257, ZINB loss: 0.96019487\n",
      "Pretrain epoch 258, ZINB loss: 0.96011013\n",
      "Pretrain epoch 259, ZINB loss: 0.96034977\n",
      "Pretrain epoch 260, ZINB loss: 0.96043889\n",
      "Pretrain epoch 261, ZINB loss: 0.96047542\n",
      "Pretrain epoch 262, ZINB loss: 0.96001097\n",
      "Pretrain epoch 263, ZINB loss: 0.96018759\n",
      "Pretrain epoch 264, ZINB loss: 0.95996801\n",
      "Pretrain epoch 265, ZINB loss: 0.96052039\n",
      "Pretrain epoch 266, ZINB loss: 0.96020630\n",
      "Pretrain epoch 267, ZINB loss: 0.95997669\n",
      "Pretrain epoch 268, ZINB loss: 0.95985673\n",
      "Pretrain epoch 269, ZINB loss: 0.96020345\n",
      "Pretrain epoch 270, ZINB loss: 0.95975665\n",
      "Pretrain epoch 271, ZINB loss: 0.95987849\n",
      "Pretrain epoch 272, ZINB loss: 0.95991694\n",
      "Pretrain epoch 273, ZINB loss: 0.96000543\n",
      "Pretrain epoch 274, ZINB loss: 0.95986064\n",
      "Pretrain epoch 275, ZINB loss: 0.95974610\n",
      "Pretrain epoch 276, ZINB loss: 0.95993841\n",
      "Pretrain epoch 277, ZINB loss: 0.96015595\n",
      "Pretrain epoch 278, ZINB loss: 0.95953637\n",
      "Pretrain epoch 279, ZINB loss: 0.95977239\n",
      "Pretrain epoch 280, ZINB loss: 0.95987015\n",
      "Pretrain epoch 281, ZINB loss: 0.95957696\n",
      "Pretrain epoch 282, ZINB loss: 0.95936381\n",
      "Pretrain epoch 283, ZINB loss: 0.95945828\n",
      "Pretrain epoch 284, ZINB loss: 0.95971139\n",
      "Pretrain epoch 285, ZINB loss: 0.95954437\n",
      "Pretrain epoch 286, ZINB loss: 0.95966395\n",
      "Pretrain epoch 287, ZINB loss: 0.95974260\n",
      "Pretrain epoch 288, ZINB loss: 0.95989295\n",
      "Pretrain epoch 289, ZINB loss: 0.95932541\n",
      "Pretrain epoch 290, ZINB loss: 0.95962897\n",
      "Pretrain epoch 291, ZINB loss: 0.95934666\n",
      "Pretrain epoch 292, ZINB loss: 0.95950862\n",
      "Pretrain epoch 293, ZINB loss: 0.95948457\n",
      "Pretrain epoch 294, ZINB loss: 0.95939633\n",
      "Pretrain epoch 295, ZINB loss: 0.95957439\n",
      "Pretrain epoch 296, ZINB loss: 0.95964326\n",
      "Pretrain epoch 297, ZINB loss: 0.96014113\n",
      "Pretrain epoch 298, ZINB loss: 0.95910497\n",
      "Pretrain epoch 299, ZINB loss: 0.95956446\n",
      "Pretrain epoch 300, ZINB loss: 0.95904611\n",
      "Pretraining time: 57 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "model.pretrain_autoencoder(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, \n",
    "                        batch_size=args.batch_size, epochs=args.pretrain_epochs, ae_weights=args.ae_weight_file)\n",
    "\n",
    "print('Pretraining time: %d seconds.' % int(time() - t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0ded9",
   "metadata": {},
   "source": [
    "Clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd033c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters:  9\n",
      "Clustering stage\n",
      "Initializing cluster centers with kmeans.\n",
      "Initializing k-means: NMI= 0.5123, ARI= 0.3989\n",
      "Clustering   : NMI= 0.4878, ARI= 0.3624\n",
      "Total time: 65 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "\n",
    "### estimate number of clusters by Louvain algorithm on the autoencoder latent representations\n",
    "pretrain_latent = model.encodeBatch(torch.tensor(adata.X)).cpu().numpy()\n",
    "adata_latent = sc.AnnData(pretrain_latent)\n",
    "sc.pp.neighbors(adata_latent, n_neighbors=args.knn, use_rep=\"X\")\n",
    "sc.tl.louvain(adata_latent, resolution=args.resolution)\n",
    "y_pred_init = np.asarray(adata_latent.obs['louvain'],dtype=int)\n",
    "features = pd.DataFrame(adata_latent.X,index=np.arange(0,adata_latent.n_obs))\n",
    "Group = pd.Series(y_pred_init,index=np.arange(0,adata_latent.n_obs),name=\"Group\")\n",
    "Mergefeature = pd.concat([features,Group],axis=1)\n",
    "cluster_centers = np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "n_clusters = cluster_centers.shape[0]\n",
    "print('Estimated number of clusters: ', n_clusters)\n",
    "y_pred, _, _, _, _ = model.fit(X=adata.X, X_raw=adata.raw.X, size_factor=adata.obs.size_factors, n_clusters=n_clusters, init_centroid=cluster_centers, \n",
    "            y_pred_init=y_pred_init, y=y, batch_size=args.batch_size, num_epochs=args.maxiter, update_interval=args.update_interval, tol=args.tol, save_dir=args.save_dir)\n",
    "\n",
    "\n",
    "print('Total time: %d seconds.' % int(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88548b5d",
   "metadata": {},
   "source": [
    "Output and save predicted labels and latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ae42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y is not None:\n",
    "    nmi, ari=cluster_acc(y, y_pred)\n",
    "\n",
    "final_latent = model.encodeBatch(torch.tensor(adata.X)).cpu().numpy()\n",
    "np.savetxt(args.final_latent_file, final_latent, delimiter=\",\")\n",
    "np.savetxt(args.predict_label_file, y_pred, delimiter=\",\", fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b8c19",
   "metadata": {},
   "source": [
    "Run t-SNE on latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60652b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "\n",
    "tsne_embedding = TSNE(\n",
    "                    n_components=2,\n",
    "                    perplexity=30,\n",
    "                    initialization=\"pca\",\n",
    "                    metric=\"euclidean\",\n",
    "                    n_jobs=8,\n",
    "                    random_state=42,\n",
    "                )\n",
    "latent_tsne_2 = tsne_embedding.fit(final_latent)\n",
    "np.savetxt(\"tsne_2D.txt\", latent_tsne_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa113e8b",
   "metadata": {},
   "source": [
    "Plot 2D t-SNE of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658bac2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rm(list = ls())\n",
    "library(ggplot2)\n",
    "\n",
    "latent_tsne <- read.table(\"tsne_2D.txt\", sep = \",\")\n",
    "colnames(latent_tsne) <- c(\"TSNE_1\", \"TSNE_2\")\n",
    "y_pred <- as.numeric(readLines(\"pred_labels.txt\"))\n",
    "y_pred <- factor(y_pred, levels = 0:max(y_pred))\n",
    "\n",
    "dat <- data.frame(latent_tsne, y_pred = y_pred)\n",
    "\n",
    "m <- ggplot(dat, aes(x = TSNE_1, y = TSNE_2, color = y_pred)) +\n",
    "    geom_point() +\n",
    "    theme_classic()\n",
    "print(m)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "06409eb1ce10e8537f90ad838e053230edeb28aa37d97c83943cd102bf0c7a79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
